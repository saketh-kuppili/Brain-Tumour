{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1b82b256",
   "metadata": {},
   "source": [
    "# Brain Tumor Classification: ResNet-50 with CBAM Attention\n",
    "\n",
    "**Objective:** Build and compare two PyTorch CNN models for trustworthy brain tumor classification from MRI images:\n",
    "1. **Baseline ResNet-50**: Standard ResNet-50 without attention mechanisms\n",
    "2. **ResNet-50 + CBAM**: ResNet-50 with Convolutional Block Attention Module (channel + spatial attention)\n",
    "\n",
    "**Dataset:** 4-class classification (glioma, meningioma, pituitary, no tumor) using PyTorch ImageFolder from the Training/ and Testing/ directories.\n",
    "\n",
    "**Key Features:**\n",
    "- CBAM implemented from scratch (channel and spatial attention)\n",
    "- Modular, production-ready PyTorch code\n",
    "- Comprehensive metrics: accuracy, per-class F1, confusion matrix\n",
    "- Training with checkpointing and LR scheduling\n",
    "- Attention map visualization on MRI images\n",
    "- Clean markdown documentation throughout"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd0e9bfd",
   "metadata": {},
   "source": [
    "## 1. Imports & Configuration\n",
    "\n",
    "Import all necessary libraries and define configuration variables for the entire experiment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1031f078",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Python executable: /Users/sakethkuppili/Desktop/brain tumour dataset/.venv/bin/python\n",
      "Python version   : 3.13.9 (v3.13.9:8183fa5e3f7, Oct 14 2025, 10:27:13) [Clang 16.0.0 (clang-1600.0.26.6)]\n",
      "\n",
      "Missing packages: ['torchvision', 'tqdm', 'sklearn', 'scipy']\n",
      "\n",
      "Install with pip (recommended):\n",
      "python -m pip install --upgrade pip\n",
      "python -m pip install torchvision tqdm sklearn scipy\n",
      "\n",
      "For PyTorch, follow the official selector (CUDA vs CPU): https://pytorch.org/get-started/locally/\n",
      "\n",
      "If you use conda, you can install with:\n",
      "conda install -y torchvision tqdm sklearn scipy\n",
      "\n",
      "Important: Ensure the Jupyter kernel is using the same Python environment shown above.\n",
      "If not, install ipykernel in that environment and add a kernel:\n",
      "python -m pip install ipykernel\n",
      "python -m ipykernel install --user --name myenv --display-name \"Python (myenv)\"\n",
      "\n",
      "After installing, restart the kernel and re-run the notebook cells.\n"
     ]
    }
   ],
   "source": [
    "# Environment diagnostic — check required packages and kernel path\n",
    "import importlib\n",
    "import sys\n",
    "\n",
    "required = ['torch', 'torchvision', 'tqdm', 'sklearn', 'scipy', 'matplotlib', 'seaborn']\n",
    "missing = []\n",
    "for pkg in required:\n",
    "    try:\n",
    "        importlib.import_module(pkg)\n",
    "    except Exception:\n",
    "        missing.append(pkg)\n",
    "\n",
    "print('Python executable:', sys.executable)\n",
    "print('Python version   :', sys.version.splitlines()[0])\n",
    "\n",
    "if not missing:\n",
    "    print('\\nAll required packages appear to be installed.')\n",
    "    try:\n",
    "        import torch\n",
    "        print('torch version:', getattr(torch, '__version__', 'unknown'))\n",
    "    except Exception:\n",
    "        pass\n",
    "else:\n",
    "    print('\\nMissing packages:', missing)\n",
    "    print('\\nInstall with pip (recommended):')\n",
    "    print('python -m pip install --upgrade pip')\n",
    "    print('python -m pip install ' + ' '.join(missing))\n",
    "    print('\\nFor PyTorch, follow the official selector (CUDA vs CPU): https://pytorch.org/get-started/locally/')\n",
    "    print('\\nIf you use conda, you can install with:')\n",
    "    print('conda install -y ' + ' '.join(missing))\n",
    "    print('\\nImportant: Ensure the Jupyter kernel is using the same Python environment shown above.')\n",
    "    print('If not, install ipykernel in that environment and add a kernel:')\n",
    "    print('python -m pip install ipykernel')\n",
    "    print(\"python -m ipykernel install --user --name myenv --display-name \\\"Python (myenv)\\\"\")\n",
    "\n",
    "print('\\nAfter installing, restart the kernel and re-run the notebook cells.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "82c42bad",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'tqdm'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mModuleNotFoundError\u001b[39m                       Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[4]\u001b[39m\u001b[32m, line 10\u001b[39m\n\u001b[32m      8\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mmatplotlib\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mpyplot\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mplt\u001b[39;00m\n\u001b[32m      9\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mseaborn\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01msns\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m10\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtqdm\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m tqdm\n\u001b[32m     12\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtorch\u001b[39;00m\n\u001b[32m     13\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtorch\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mnn\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mnn\u001b[39;00m\n",
      "\u001b[31mModuleNotFoundError\u001b[39m: No module named 'tqdm'"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import json\n",
    "import random\n",
    "from pathlib import Path\n",
    "from collections import defaultdict\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from tqdm import tqdm\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.optim.lr_scheduler import CosineAnnealingLR\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import datasets, models, transforms\n",
    "from sklearn.metrics import confusion_matrix, precision_recall_fscore_support, accuracy_score\n",
    "\n",
    "# Set style for visualizations\n",
    "plt.style.use('seaborn-v0_8-darkgrid')\n",
    "sns.set_palette(\"husl\")\n",
    "\n",
    "# Device configuration\n",
    "DEVICE = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"Using device: {DEVICE}\")\n",
    "\n",
    "# ===== Configuration =====\n",
    "CONFIG = {\n",
    "    'batch_size': 32,\n",
    "    'learning_rate': 1e-3,\n",
    "    'weight_decay': 1e-5,\n",
    "    'epochs': 50,\n",
    "    'num_workers': 4,\n",
    "    'image_size': 224,\n",
    "    'num_classes': 4,\n",
    "    'checkpoint_dir': './checkpoints',\n",
    "    'seed': 42,\n",
    "    'cbam_reduction': 16,  # Reduction ratio for CBAM channel attention\n",
    "    'pretrained': False,  # Start with random weights for fair comparison\n",
    "}\n",
    "\n",
    "# Create checkpoint directory\n",
    "os.makedirs(CONFIG['checkpoint_dir'], exist_ok=True)\n",
    "print(f\"Config: {json.dumps(CONFIG, indent=2)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69cc759e",
   "metadata": {},
   "source": [
    "## 2. Reproducibility & Utilities\n",
    "\n",
    "Set deterministic seeds, implement checkpoint saving/loading, and utility functions for training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0078652",
   "metadata": {},
   "outputs": [],
   "source": [
    "def set_seed(seed: int):\n",
    "    \"\"\"Set random seeds for reproducibility.\"\"\"\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.manual_seed_all(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "\n",
    "def to_device(batch, device):\n",
    "    \"\"\"Move batch (images, labels) to specified device.\"\"\"\n",
    "    images, labels = batch\n",
    "    return images.to(device), labels.to(device)\n",
    "\n",
    "def save_checkpoint(model, optimizer, scheduler, epoch, best_metric, model_name, config):\n",
    "    \"\"\"Save model checkpoint with all state.\"\"\"\n",
    "    checkpoint = {\n",
    "        'epoch': epoch,\n",
    "        'model_state': model.state_dict(),\n",
    "        'optimizer_state': optimizer.state_dict(),\n",
    "        'scheduler_state': scheduler.state_dict() if scheduler else None,\n",
    "        'best_metric': best_metric,\n",
    "        'config': config,\n",
    "    }\n",
    "    path = os.path.join(config['checkpoint_dir'], f\"{model_name}_best.pt\")\n",
    "    torch.save(checkpoint, path)\n",
    "    print(f\"✓ Checkpoint saved: {path}\")\n",
    "\n",
    "def load_checkpoint(model, optimizer, scheduler, model_name, config, device):\n",
    "    \"\"\"Load model checkpoint and restore state.\"\"\"\n",
    "    path = os.path.join(config['checkpoint_dir'], f\"{model_name}_best.pt\")\n",
    "    if os.path.exists(path):\n",
    "        checkpoint = torch.load(path, map_location=device)\n",
    "        model.load_state_dict(checkpoint['model_state'])\n",
    "        optimizer.load_state_dict(checkpoint['optimizer_state'])\n",
    "        if scheduler and checkpoint['scheduler_state']:\n",
    "            scheduler.load_state_dict(checkpoint['scheduler_state'])\n",
    "        print(f\"✓ Checkpoint loaded from epoch {checkpoint['epoch']}, best_metric={checkpoint['best_metric']:.4f}\")\n",
    "        return checkpoint['epoch'], checkpoint['best_metric']\n",
    "    return 0, 0.0\n",
    "\n",
    "class MetricsLogger:\n",
    "    \"\"\"Simple training history logger.\"\"\"\n",
    "    def __init__(self):\n",
    "        self.history = defaultdict(list)\n",
    "    \n",
    "    def log(self, **kwargs):\n",
    "        for key, value in kwargs.items():\n",
    "            self.history[key].append(value)\n",
    "    \n",
    "    def save(self, path):\n",
    "        with open(path, 'w') as f:\n",
    "            json.dump(self.history, f, indent=2)\n",
    "    \n",
    "    def plot_curves(self, figsize=(14, 5)):\n",
    "        fig, axes = plt.subplots(1, 2, figsize=figsize)\n",
    "        \n",
    "        if 'train_loss' in self.history and 'val_loss' in self.history:\n",
    "            axes[0].plot(self.history['train_loss'], label='Train Loss', marker='o')\n",
    "            axes[0].plot(self.history['val_loss'], label='Val Loss', marker='s')\n",
    "            axes[0].set_xlabel('Epoch')\n",
    "            axes[0].set_ylabel('Loss')\n",
    "            axes[0].set_title('Training & Validation Loss')\n",
    "            axes[0].legend()\n",
    "            axes[0].grid(True, alpha=0.3)\n",
    "        \n",
    "        if 'train_acc' in self.history and 'val_acc' in self.history:\n",
    "            axes[1].plot(self.history['train_acc'], label='Train Accuracy', marker='o')\n",
    "            axes[1].plot(self.history['val_acc'], label='Val Accuracy', marker='s')\n",
    "            axes[1].set_xlabel('Epoch')\n",
    "            axes[1].set_ylabel('Accuracy')\n",
    "            axes[1].set_title('Training & Validation Accuracy')\n",
    "            axes[1].legend()\n",
    "            axes[1].grid(True, alpha=0.3)\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        return fig\n",
    "\n",
    "# Set seed for reproducibility\n",
    "set_seed(CONFIG['seed'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a89bca2b",
   "metadata": {},
   "source": [
    "## 3. Data Transforms & Dataset Loading\n",
    "\n",
    "Define image transformations and load datasets using PyTorch's ImageFolder."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b0084cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Image Normalization Stats (ImageNet)\n",
    "IMAGENET_MEAN = [0.485, 0.456, 0.406]\n",
    "IMAGENET_STD = [0.229, 0.224, 0.225]\n",
    "\n",
    "# Training transforms: augmentation for robustness\n",
    "train_transforms = transforms.Compose([\n",
    "    transforms.RandomRotation(20),\n",
    "    transforms.RandomHorizontalFlip(p=0.5),\n",
    "    transforms.RandomAffine(degrees=0, translate=(0.1, 0.1)),\n",
    "    transforms.Resize((CONFIG['image_size'], CONFIG['image_size'])),\n",
    "    transforms.CenterCrop(CONFIG['image_size']),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=IMAGENET_MEAN, std=IMAGENET_STD),\n",
    "])\n",
    "\n",
    "# Test transforms: deterministic, minimal augmentation\n",
    "test_transforms = transforms.Compose([\n",
    "    transforms.Resize((CONFIG['image_size'], CONFIG['image_size'])),\n",
    "    transforms.CenterCrop(CONFIG['image_size']),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=IMAGENET_MEAN, std=IMAGENET_STD),\n",
    "])\n",
    "\n",
    "# Load datasets using ImageFolder\n",
    "train_dataset = datasets.ImageFolder(\n",
    "    root='./Training',\n",
    "    transform=train_transforms\n",
    ")\n",
    "\n",
    "test_dataset = datasets.ImageFolder(\n",
    "    root='./Testing',\n",
    "    transform=test_transforms\n",
    ")\n",
    "\n",
    "# Class mappings\n",
    "class_to_idx = train_dataset.class_to_idx\n",
    "idx_to_class = {v: k for k, v in class_to_idx.items()}\n",
    "\n",
    "print(f\"✓ Training samples: {len(train_dataset)}\")\n",
    "print(f\"✓ Testing samples: {len(test_dataset)}\")\n",
    "print(f\"✓ Classes: {list(idx_to_class.values())}\")\n",
    "print(f\"✓ Class mapping: {class_to_idx}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "303998f2",
   "metadata": {},
   "source": [
    "## 4. DataLoaders & Sample Visualization\n",
    "\n",
    "Create data loaders and visualize sample MRI images to verify data loading."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53c862f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create data loaders\n",
    "train_loader = DataLoader(\n",
    "    train_dataset,\n",
    "    batch_size=CONFIG['batch_size'],\n",
    "    shuffle=True,\n",
    "    num_workers=CONFIG['num_workers'],\n",
    "    pin_memory=True,\n",
    ")\n",
    "\n",
    "test_loader = DataLoader(\n",
    "    test_dataset,\n",
    "    batch_size=CONFIG['batch_size'],\n",
    "    shuffle=False,\n",
    "    num_workers=CONFIG['num_workers'],\n",
    "    pin_memory=True,\n",
    ")\n",
    "\n",
    "print(f\"✓ Train batches: {len(train_loader)}\")\n",
    "print(f\"✓ Test batches: {len(test_loader)}\")\n",
    "\n",
    "# Visualize sample images\n",
    "def plot_sample_batch(dataloader, idx_to_class, num_samples=8):\n",
    "    \"\"\"Visualize a batch of MRI images.\"\"\"\n",
    "    images, labels = next(iter(dataloader))\n",
    "    \n",
    "    # Denormalize for visualization\n",
    "    images = images * torch.tensor(IMAGENET_STD).view(1, 3, 1, 1) + torch.tensor(IMAGENET_MEAN).view(1, 3, 1, 1)\n",
    "    images = torch.clamp(images, 0, 1)\n",
    "    \n",
    "    num_samples = min(num_samples, len(images))\n",
    "    fig, axes = plt.subplots(2, 4, figsize=(14, 6))\n",
    "    axes = axes.flatten()\n",
    "    \n",
    "    for i in range(num_samples):\n",
    "        ax = axes[i]\n",
    "        img = images[i].permute(1, 2, 0).numpy()\n",
    "        ax.imshow(img, cmap='gray')\n",
    "        ax.set_title(f\"Class: {idx_to_class[labels[i].item()]}\", fontsize=10)\n",
    "        ax.axis('off')\n",
    "    \n",
    "    plt.suptitle(\"Sample Training Images\", fontsize=14, fontweight='bold')\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "plot_sample_batch(train_loader, idx_to_class)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9179c9fc",
   "metadata": {},
   "source": [
    "## 5. CBAM Implementation (from Scratch)\n",
    "\n",
    "Implement Convolutional Block Attention Module with channel and spatial attention mechanisms.\n",
    "\n",
    "### Channel Attention\n",
    "For each channel, compute attention weights using both global average and max pooling:\n",
    "$$\\text{CA}(F) = \\sigma(MLP(AvgPool(F)) + MLP(MaxPool(F)))$$\n",
    "\n",
    "### Spatial Attention\n",
    "Compute attention maps by concatenating average and max pooling across the channel dimension:\n",
    "$$\\text{SA}(F) = \\sigma(f^{7 \\times 7}([\\text{AvgPool}(F); \\text{MaxPool}(F)]))$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22450d14",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ChannelAttention(nn.Module):\n",
    "    \"\"\"Channel Attention Module: learns which channels are important.\"\"\"\n",
    "    def __init__(self, num_channels, reduction=16):\n",
    "        super(ChannelAttention, self).__init__()\n",
    "        self.avg_pool = nn.AdaptiveAvgPool2d(1)\n",
    "        self.max_pool = nn.AdaptiveMaxPool2d(1)\n",
    "        \n",
    "        # Shared MLP with reduction\n",
    "        self.fc = nn.Sequential(\n",
    "            nn.Linear(num_channels, num_channels // reduction, bias=False),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Linear(num_channels // reduction, num_channels, bias=False),\n",
    "        )\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "    \n",
    "    def forward(self, x):\n",
    "        # x shape: (B, C, H, W)\n",
    "        avg_out = self.fc(self.avg_pool(x).view(x.size(0), -1))\n",
    "        max_out = self.fc(self.max_pool(x).view(x.size(0), -1))\n",
    "        out = avg_out + max_out\n",
    "        return self.sigmoid(out).view(x.size(0), -1, 1, 1)\n",
    "\n",
    "class SpatialAttention(nn.Module):\n",
    "    \"\"\"Spatial Attention Module: learns which spatial regions are important.\"\"\"\n",
    "    def __init__(self, kernel_size=7):\n",
    "        super(SpatialAttention, self).__init__()\n",
    "        padding = (kernel_size - 1) // 2\n",
    "        self.conv = nn.Conv2d(2, 1, kernel_size=kernel_size, padding=padding, bias=False)\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "    \n",
    "    def forward(self, x):\n",
    "        # x shape: (B, C, H, W)\n",
    "        avg_out = torch.mean(x, dim=1, keepdim=True)\n",
    "        max_out, _ = torch.max(x, dim=1, keepdim=True)\n",
    "        x_cat = torch.cat([avg_out, max_out], dim=1)\n",
    "        out = self.conv(x_cat)\n",
    "        return self.sigmoid(out)\n",
    "\n",
    "class CBAM(nn.Module):\n",
    "    \"\"\"Convolutional Block Attention Module: combines channel and spatial attention.\"\"\"\n",
    "    def __init__(self, num_channels, reduction=16, kernel_size=7):\n",
    "        super(CBAM, self).__init__()\n",
    "        self.channel_attention = ChannelAttention(num_channels, reduction)\n",
    "        self.spatial_attention = SpatialAttention(kernel_size)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        # Apply channel attention\n",
    "        ca_out = x * self.channel_attention(x)\n",
    "        # Apply spatial attention\n",
    "        sa_out = ca_out * self.spatial_attention(ca_out)\n",
    "        return sa_out\n",
    "    \n",
    "    def get_spatial_attention(self, x):\n",
    "        \"\"\"Extract spatial attention map (for visualization).\"\"\"\n",
    "        with torch.no_grad():\n",
    "            ca_out = x * self.channel_attention(x)\n",
    "            sa_map = self.spatial_attention(ca_out)\n",
    "        return sa_map\n",
    "\n",
    "print(\"✓ CBAM modules defined (Channel Attention + Spatial Attention)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d965b5f4",
   "metadata": {},
   "source": [
    "## 6. ResNet-50 with CBAM Integration\n",
    "\n",
    "Modify ResNet-50 bottleneck blocks to include CBAM attention after each block."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4c5eff7",
   "metadata": {},
   "outputs": [],
   "source": [
    "class BottleneckWithCBAM(nn.Module):\n",
    "    \"\"\"Bottleneck block with CBAM attention.\"\"\"\n",
    "    expansion = 4\n",
    "    \n",
    "    def __init__(self, original_bottleneck, cbam_module):\n",
    "        super(BottleneckWithCBAM, self).__init__()\n",
    "        self.bottleneck = original_bottleneck\n",
    "        self.cbam = cbam_module\n",
    "    \n",
    "    def forward(self, x):\n",
    "        identity = x\n",
    "        out = self.bottleneck.conv1(x)\n",
    "        out = self.bottleneck.bn1(out)\n",
    "        out = self.bottleneck.relu(out)\n",
    "        \n",
    "        out = self.bottleneck.conv2(out)\n",
    "        out = self.bottleneck.bn2(out)\n",
    "        out = self.bottleneck.relu(out)\n",
    "        \n",
    "        out = self.bottleneck.conv3(out)\n",
    "        out = self.bottleneck.bn3(out)\n",
    "        \n",
    "        # Shortcut connection\n",
    "        if self.bottleneck.downsample is not None:\n",
    "            identity = self.bottleneck.downsample(x)\n",
    "        \n",
    "        out += identity\n",
    "        out = self.bottleneck.relu(out)\n",
    "        \n",
    "        # Apply CBAM after residual block\n",
    "        out = self.cbam(out)\n",
    "        return out\n",
    "\n",
    "def add_cbam_to_resnet50(model, reduction=16):\n",
    "    \"\"\"Add CBAM modules to ResNet-50 bottleneck blocks.\"\"\"\n",
    "    for layer in [model.layer1, model.layer2, model.layer3, model.layer4]:\n",
    "        for i, block in enumerate(layer):\n",
    "            # Get the output channels of the bottleneck\n",
    "            out_channels = block.conv3.out_channels\n",
    "            cbam = CBAM(out_channels, reduction=reduction)\n",
    "            # Wrap bottleneck with CBAM\n",
    "            layer[i] = BottleneckWithCBAM(block, cbam)\n",
    "    return model\n",
    "\n",
    "print(\"✓ ResNet-50 + CBAM wrapper defined\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf7f8238",
   "metadata": {},
   "source": [
    "## 7. Model Factory\n",
    "\n",
    "Create a factory function to build baseline and CBAM-augmented models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63b3a5f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_model(mode='baseline', num_classes=4, pretrained=False, cbam_reduction=16):\n",
    "    \"\"\"\n",
    "    Build ResNet-50 model with or without CBAM attention.\n",
    "    \n",
    "    Args:\n",
    "        mode: 'baseline' or 'cbam'\n",
    "        num_classes: Number of output classes\n",
    "        pretrained: Use pretrained ImageNet weights\n",
    "        cbam_reduction: Reduction ratio for CBAM channel attention\n",
    "    \n",
    "    Returns:\n",
    "        model, parameter count\n",
    "    \"\"\"\n",
    "    model = models.resnet50(pretrained=pretrained, weights='DEFAULT' if pretrained else None)\n",
    "    \n",
    "    if mode == 'cbam':\n",
    "        model = add_cbam_to_resnet50(model, reduction=cbam_reduction)\n",
    "    \n",
    "    # Replace final classification layer\n",
    "    in_features = model.fc.in_features\n",
    "    model.fc = nn.Linear(in_features, num_classes)\n",
    "    \n",
    "    # Move to device\n",
    "    model = model.to(DEVICE)\n",
    "    \n",
    "    # Count parameters\n",
    "    total_params = sum(p.numel() for p in model.parameters())\n",
    "    trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "    \n",
    "    print(f\"✓ Model ({mode.upper()}) built:\")\n",
    "    print(f\"  - Total params: {total_params:,}\")\n",
    "    print(f\"  - Trainable params: {trainable_params:,}\")\n",
    "    \n",
    "    return model, total_params\n",
    "\n",
    "# Build both models\n",
    "model_baseline, params_baseline = build_model(mode='baseline', num_classes=CONFIG['num_classes'], pretrained=CONFIG['pretrained'])\n",
    "model_cbam, params_cbam = build_model(mode='cbam', num_classes=CONFIG['num_classes'], pretrained=CONFIG['pretrained'], cbam_reduction=CONFIG['cbam_reduction'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4cd05d2c",
   "metadata": {},
   "source": [
    "## 8. Metrics & Loss Functions\n",
    "\n",
    "Define loss, accuracy, and per-class F1 scores.\n",
    "\n",
    "### Per-Class F1 Score\n",
    "$$F1 = \\frac{2 \\cdot TP}{2 \\cdot TP + FP + FN}$$\n",
    "\n",
    "where TP = True Positives, FP = False Positives, FN = False Negatives"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b1b5be6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loss function\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "def compute_metrics(y_true, y_pred, idx_to_class):\n",
    "    \"\"\"\n",
    "    Compute accuracy, per-class precision, recall, F1.\n",
    "    \n",
    "    Args:\n",
    "        y_true: Ground truth labels (numpy array)\n",
    "        y_pred: Predicted labels (numpy array)\n",
    "        idx_to_class: Class index to name mapping\n",
    "    \n",
    "    Returns:\n",
    "        Dictionary with metrics\n",
    "    \"\"\"\n",
    "    cm = confusion_matrix(y_true, y_pred, labels=list(range(len(idx_to_class))))\n",
    "    precision, recall, f1, _ = precision_recall_fscore_support(\n",
    "        y_true, y_pred, labels=list(range(len(idx_to_class))), average=None\n",
    "    )\n",
    "    accuracy = accuracy_score(y_true, y_pred)\n",
    "    \n",
    "    metrics = {\n",
    "        'accuracy': accuracy,\n",
    "        'confusion_matrix': cm,\n",
    "        'per_class': {}\n",
    "    }\n",
    "    \n",
    "    for i, class_name in idx_to_class.items():\n",
    "        metrics['per_class'][class_name] = {\n",
    "            'precision': float(precision[i]),\n",
    "            'recall': float(recall[i]),\n",
    "            'f1': float(f1[i]),\n",
    "            'support': int(cm[i].sum())\n",
    "        }\n",
    "    \n",
    "    # Macro-averaged F1\n",
    "    metrics['macro_f1'] = float(f1.mean())\n",
    "    \n",
    "    return metrics\n",
    "\n",
    "def print_metrics(metrics, idx_to_class):\n",
    "    \"\"\"Pretty-print metrics.\"\"\"\n",
    "    print(f\"\\n{'='*70}\")\n",
    "    print(f\"Overall Accuracy: {metrics['accuracy']:.4f}\")\n",
    "    print(f\"Macro-Averaged F1: {metrics['macro_f1']:.4f}\")\n",
    "    print(f\"\\n{'Class':<15} {'Precision':<12} {'Recall':<12} {'F1':<12} {'Support':<10}\")\n",
    "    print(f\"{'-'*70}\")\n",
    "    for i, class_name in idx_to_class.items():\n",
    "        stats = metrics['per_class'][class_name]\n",
    "        print(f\"{class_name:<15} {stats['precision']:<12.4f} {stats['recall']:<12.4f} {stats['f1']:<12.4f} {stats['support']:<10}\")\n",
    "    print(f\"{'='*70}\\n\")\n",
    "\n",
    "print(\"✓ Metrics functions defined\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48828c11",
   "metadata": {},
   "source": [
    "## 9. Training Loop\n",
    "\n",
    "Train both models with progress tracking, checkpointing, and per-epoch logging."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13549ffc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_one_epoch(model, dataloader, criterion, optimizer, device, idx_to_class):\n",
    "    \"\"\"\n",
    "    Train for one epoch.\n",
    "    \n",
    "    Returns:\n",
    "        Dictionary with training metrics\n",
    "    \"\"\"\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "    all_preds = []\n",
    "    all_labels = []\n",
    "    \n",
    "    pbar = tqdm(dataloader, desc=\"Training\", leave=False)\n",
    "    for images, labels in pbar:\n",
    "        images, labels = to_device((images, labels), device)\n",
    "        \n",
    "        # Forward pass\n",
    "        outputs = model(images)\n",
    "        loss = criterion(outputs, labels)\n",
    "        \n",
    "        # Backward pass\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        # Accumulate metrics\n",
    "        running_loss += loss.item() * labels.size(0)\n",
    "        preds = outputs.argmax(dim=1)\n",
    "        all_preds.extend(preds.cpu().numpy())\n",
    "        all_labels.extend(labels.cpu().numpy())\n",
    "        \n",
    "        pbar.set_postfix({'loss': loss.item():.4f})\n",
    "    \n",
    "    # Compute epoch metrics\n",
    "    all_preds = np.array(all_preds)\n",
    "    all_labels = np.array(all_labels)\n",
    "    epoch_loss = running_loss / len(dataloader.dataset)\n",
    "    metrics = compute_metrics(all_labels, all_preds, idx_to_class)\n",
    "    \n",
    "    return {\n",
    "        'loss': epoch_loss,\n",
    "        'accuracy': metrics['accuracy'],\n",
    "        'macro_f1': metrics['macro_f1'],\n",
    "        'metrics': metrics\n",
    "    }\n",
    "\n",
    "def evaluate(model, dataloader, criterion, device, idx_to_class):\n",
    "    \"\"\"\n",
    "    Evaluate on validation/test set.\n",
    "    \n",
    "    Returns:\n",
    "        Dictionary with evaluation metrics\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "    running_loss = 0.0\n",
    "    all_preds = []\n",
    "    all_labels = []\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for images, labels in tqdm(dataloader, desc=\"Evaluating\", leave=False):\n",
    "            images, labels = to_device((images, labels), device)\n",
    "            \n",
    "            outputs = model(images)\n",
    "            loss = criterion(outputs, labels)\n",
    "            \n",
    "            running_loss += loss.item() * labels.size(0)\n",
    "            preds = outputs.argmax(dim=1)\n",
    "            all_preds.extend(preds.cpu().numpy())\n",
    "            all_labels.extend(labels.cpu().numpy())\n",
    "    \n",
    "    # Compute metrics\n",
    "    all_preds = np.array(all_preds)\n",
    "    all_labels = np.array(all_labels)\n",
    "    epoch_loss = running_loss / len(dataloader.dataset)\n",
    "    metrics = compute_metrics(all_labels, all_preds, idx_to_class)\n",
    "    \n",
    "    return {\n",
    "        'loss': epoch_loss,\n",
    "        'accuracy': metrics['accuracy'],\n",
    "        'macro_f1': metrics['macro_f1'],\n",
    "        'metrics': metrics\n",
    "    }\n",
    "\n",
    "print(\"✓ Training and evaluation functions defined\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "947a3fac",
   "metadata": {},
   "source": [
    "## 10. CBAM Attention Visualization\n",
    "\n",
    "Visualize spatial attention maps overlaid on original MRI images."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e7083c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_cbam_attention_maps(model, dataloader, device, idx_to_class, num_samples=5):\n",
    "    \"\"\"\n",
    "    Extract CBAM spatial attention maps from layer4 for visualization.\n",
    "    \n",
    "    Args:\n",
    "        model: ResNet-50 with CBAM (must have BottleneckWithCBAM in layer4)\n",
    "        dataloader: DataLoader for extracting samples\n",
    "        device: torch device\n",
    "        idx_to_class: Class index mapping\n",
    "        num_samples: Number of samples to extract\n",
    "    \n",
    "    Returns:\n",
    "        Dictionary with images, predictions, labels, and attention maps\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "    \n",
    "    # Hook to capture attention maps\n",
    "    attention_maps = []\n",
    "    \n",
    "    def hook_fn(module, input, output):\n",
    "        if hasattr(module, 'cbam'):\n",
    "            # Get spatial attention map from CBAM\n",
    "            x = input[0]\n",
    "            with torch.no_grad():\n",
    "                ca_out = x * module.cbam.channel_attention(x)\n",
    "                sa_map = module.cbam.spatial_attention(ca_out)\n",
    "                attention_maps.append(sa_map.cpu())\n",
    "    \n",
    "    # Register hooks on layer4 blocks\n",
    "    handles = []\n",
    "    for block in model.layer4:\n",
    "        h = block.register_forward_hook(hook_fn)\n",
    "        handles.append(h)\n",
    "    \n",
    "    results = {\n",
    "        'images': [],\n",
    "        'predictions': [],\n",
    "        'labels': [],\n",
    "        'attention_maps': [],\n",
    "        'class_names': []\n",
    "    }\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for images, labels in dataloader:\n",
    "            if len(results['images']) >= num_samples:\n",
    "                break\n",
    "            \n",
    "            images_orig = images.clone()\n",
    "            images, labels = to_device((images, labels), device)\n",
    "            \n",
    "            attention_maps.clear()\n",
    "            outputs = model(images)\n",
    "            preds = outputs.argmax(dim=1)\n",
    "            \n",
    "            for i in range(min(len(images), num_samples - len(results['images']))):\n",
    "                results['images'].append(images_orig[i])\n",
    "                results['predictions'].append(idx_to_class[preds[i].item()])\n",
    "                results['labels'].append(idx_to_class[labels[i].item()])\n",
    "                results['class_names'].append(idx_to_class[preds[i].item()])\n",
    "                \n",
    "                # Use last attention map from layer4\n",
    "                if attention_maps:\n",
    "                    att_map = attention_maps[-1][i:i+1]\n",
    "                    results['attention_maps'].append(att_map)\n",
    "    \n",
    "    # Remove hooks\n",
    "    for h in handles:\n",
    "        h.remove()\n",
    "    \n",
    "    return results\n",
    "\n",
    "def visualize_attention(results, idx_to_class, figsize=(16, 10)):\n",
    "    \"\"\"\n",
    "    Visualize original images with overlaid CBAM attention maps.\n",
    "    \n",
    "    Args:\n",
    "        results: Dictionary from get_cbam_attention_maps\n",
    "        idx_to_class: Class mapping\n",
    "        figsize: Figure size\n",
    "    \"\"\"\n",
    "    num_samples = len(results['images'])\n",
    "    fig, axes = plt.subplots(num_samples, 3, figsize=figsize)\n",
    "    if num_samples == 1:\n",
    "        axes = axes.reshape(1, -1)\n",
    "    \n",
    "    for i in range(num_samples):\n",
    "        # Original image (denormalized)\n",
    "        img = results['images'][i]\n",
    "        img = img * torch.tensor(IMAGENET_STD).view(3, 1, 1) + torch.tensor(IMAGENET_MEAN).view(3, 1, 1)\n",
    "        img = torch.clamp(img, 0, 1).permute(1, 2, 0).numpy()\n",
    "        \n",
    "        axes[i, 0].imshow(img, cmap='gray')\n",
    "        axes[i, 0].set_title(f\"Original\\nTrue: {results['labels'][i]}\\nPred: {results['predictions'][i]}\", fontsize=10)\n",
    "        axes[i, 0].axis('off')\n",
    "        \n",
    "        # Attention map\n",
    "        if results['attention_maps']:\n",
    "            att_map = results['attention_maps'][i].squeeze().numpy()\n",
    "            att_map = (att_map - att_map.min()) / (att_map.max() - att_map.min() + 1e-8)\n",
    "            axes[i, 1].imshow(att_map, cmap='hot')\n",
    "            axes[i, 1].set_title(f\"CBAM Attention Map\", fontsize=10)\n",
    "            axes[i, 1].axis('off')\n",
    "            \n",
    "            # Overlay: resize attention map to match image\n",
    "            from scipy.ndimage import zoom\n",
    "            att_resized = zoom(att_map, (img.shape[0] / att_map.shape[0], img.shape[1] / att_map.shape[1]), order=1)\n",
    "            overlay = img.copy()\n",
    "            overlay[..., 0] = 0.5 * img[..., 0] + 0.5 * att_resized\n",
    "            axes[i, 2].imshow(overlay, cmap='gray')\n",
    "            axes[i, 2].set_title(f\"Attention Overlay\", fontsize=10)\n",
    "            axes[i, 2].axis('off')\n",
    "        else:\n",
    "            axes[i, 1].text(0.5, 0.5, 'No attention data', ha='center', va='center')\n",
    "            axes[i, 1].axis('off')\n",
    "            axes[i, 2].axis('off')\n",
    "    \n",
    "    plt.suptitle(\"CBAM Spatial Attention Visualization on MRI Images\", fontsize=14, fontweight='bold')\n",
    "    plt.tight_layout()\n",
    "    return fig\n",
    "\n",
    "print(\"✓ Attention visualization functions defined\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "819bf7e3",
   "metadata": {},
   "source": [
    "## 11. Confusion Matrix Visualization\n",
    "\n",
    "Plot confusion matrices for model evaluation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0fcce23d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_confusion_matrix(cm, idx_to_class, title='Confusion Matrix', normalize=True):\n",
    "    \"\"\"\n",
    "    Plot confusion matrix.\n",
    "    \n",
    "    Args:\n",
    "        cm: Confusion matrix from sklearn\n",
    "        idx_to_class: Class mapping\n",
    "        title: Plot title\n",
    "        normalize: Whether to normalize by row\n",
    "    \"\"\"\n",
    "    if normalize:\n",
    "        cm = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n",
    "        fmt = '.2%'\n",
    "    else:\n",
    "        fmt = 'd'\n",
    "    \n",
    "    fig, ax = plt.subplots(figsize=(8, 7))\n",
    "    class_names = [idx_to_class[i] for i in range(len(idx_to_class))]\n",
    "    \n",
    "    sns.heatmap(cm, annot=True, fmt=fmt, cmap='Blues', \n",
    "                xticklabels=class_names, \n",
    "                yticklabels=class_names,\n",
    "                cbar_kws={'label': 'Accuracy' if normalize else 'Count'},\n",
    "                ax=ax)\n",
    "    ax.set_xlabel('Predicted Label', fontsize=12)\n",
    "    ax.set_ylabel('True Label', fontsize=12)\n",
    "    ax.set_title(title, fontsize=14, fontweight='bold')\n",
    "    plt.tight_layout()\n",
    "    return fig\n",
    "\n",
    "print(\"✓ Confusion matrix visualization defined\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "943fcfc0",
   "metadata": {},
   "source": [
    "## 12. Experiment Runner: Train & Compare Both Models\n",
    "\n",
    "Train both baseline and CBAM models on the same dataset and compare results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cdd3a3c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_experiment(model, model_name, train_loader, test_loader, config, idx_to_class):\n",
    "    \"\"\"\n",
    "    Train and evaluate a model, logging metrics and saving best checkpoint.\n",
    "    \"\"\"\n",
    "    optimizer = optim.Adam(model.parameters(), lr=config['learning_rate'], weight_decay=config['weight_decay'])\n",
    "    scheduler = CosineAnnealingLR(optimizer, T_max=config['epochs'])\n",
    "    logger = MetricsLogger()\n",
    "    best_f1 = 0.0\n",
    "    best_epoch = 0\n",
    "    \n",
    "    for epoch in range(1, config['epochs'] + 1):\n",
    "        print(f\"\\nEpoch {epoch}/{config['epochs']} [{model_name}]\")\n",
    "        train_metrics = train_one_epoch(model, train_loader, criterion, optimizer, DEVICE, idx_to_class)\n",
    "        val_metrics = evaluate(model, test_loader, criterion, DEVICE, idx_to_class)\n",
    "        scheduler.step()\n",
    "        \n",
    "        logger.log(\n",
    "            train_loss=train_metrics['loss'],\n",
    "            train_acc=train_metrics['accuracy'],\n",
    "            train_f1=train_metrics['macro_f1'],\n",
    "            val_loss=val_metrics['loss'],\n",
    "            val_acc=val_metrics['accuracy'],\n",
    "            val_f1=val_metrics['macro_f1']\n",
    "        )\n",
    "        \n",
    "        print(f\"Train Loss: {train_metrics['loss']:.4f} | Acc: {train_metrics['accuracy']:.4f} | F1: {train_metrics['macro_f1']:.4f}\")\n",
    "        print(f\"Val   Loss: {val_metrics['loss']:.4f} | Acc: {val_metrics['accuracy']:.4f} | F1: {val_metrics['macro_f1']:.4f}\")\n",
    "        \n",
    "        # Save best model by validation F1\n",
    "        if val_metrics['macro_f1'] > best_f1:\n",
    "            best_f1 = val_metrics['macro_f1']\n",
    "            best_epoch = epoch\n",
    "            save_checkpoint(model, optimizer, scheduler, epoch, best_f1, model_name, config)\n",
    "    \n",
    "    print(f\"\\nBest Val F1: {best_f1:.4f} at epoch {best_epoch}\")\n",
    "    logger.save(os.path.join(config['checkpoint_dir'], f\"{model_name}_history.json\"))\n",
    "    return logger\n",
    "\n",
    "# Run experiments for both models\n",
    "logger_baseline = run_experiment(model_baseline, 'baseline', train_loader, test_loader, CONFIG, idx_to_class)\n",
    "logger_cbam = run_experiment(model_cbam, 'cbam', train_loader, test_loader, CONFIG, idx_to_class)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57eedf36",
   "metadata": {},
   "source": [
    "## 13. Plot Training Curves & Compare Results\n",
    "\n",
    "Visualize training/validation loss and accuracy for both models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5788adb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot training curves for both models\n",
    "fig1 = logger_baseline.plot_curves()\n",
    "fig1.suptitle('Baseline ResNet-50 Training Curves', fontsize=16)\n",
    "fig2 = logger_cbam.plot_curves()\n",
    "fig2.suptitle('ResNet-50 + CBAM Training Curves', fontsize=16)\n",
    "plt.show()\n",
    "\n",
    "# Evaluate and plot confusion matrices for both models\n",
    "val_metrics_baseline = evaluate(model_baseline, test_loader, criterion, DEVICE, idx_to_class)['metrics']\n",
    "val_metrics_cbam = evaluate(model_cbam, test_loader, criterion, DEVICE, idx_to_class)['metrics']\n",
    "\n",
    "fig3 = plot_confusion_matrix(val_metrics_baseline['confusion_matrix'], idx_to_class, title='Baseline ResNet-50 Confusion Matrix')\n",
    "fig4 = plot_confusion_matrix(val_metrics_cbam['confusion_matrix'], idx_to_class, title='ResNet-50 + CBAM Confusion Matrix')\n",
    "plt.show()\n",
    "\n",
    "print_metrics(val_metrics_baseline, idx_to_class)\n",
    "print_metrics(val_metrics_cbam, idx_to_class)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bdbb5497",
   "metadata": {},
   "source": [
    "## 14. Example Inference & Saving Artifacts\n",
    "\n",
    "Demonstrate loading a saved checkpoint, running inference, and saving model/metrics for reproducibility."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e2f22ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example: Load best CBAM model and run inference on test images\n",
    "model_cbam_loaded, _ = build_model(mode='cbam', num_classes=CONFIG['num_classes'], pretrained=CONFIG['pretrained'], cbam_reduction=CONFIG['cbam_reduction'])\n",
    "optimizer_cbam = optim.Adam(model_cbam_loaded.parameters(), lr=CONFIG['learning_rate'], weight_decay=CONFIG['weight_decay'])\n",
    "scheduler_cbam = CosineAnnealingLR(optimizer_cbam, T_max=CONFIG['epochs'])\n",
    "load_checkpoint(model_cbam_loaded, optimizer_cbam, scheduler_cbam, 'cbam', CONFIG, DEVICE)\n",
    "\n",
    "# Inference on a few test images\n",
    "model_cbam_loaded.eval()\n",
    "with torch.no_grad():\n",
    "    images, labels = next(iter(test_loader))\n",
    "    images, labels = to_device((images, labels), DEVICE)\n",
    "    outputs = model_cbam_loaded(images)\n",
    "    probs = F.softmax(outputs, dim=1)\n",
    "    preds = outputs.argmax(dim=1)\n",
    "    for i in range(5):\n",
    "        print(f\"Image {i+1}: True={idx_to_class[labels[i].item()]}, Pred={idx_to_class[preds[i].item()]}, Prob={probs[i][preds[i]].item():.4f}\")\n",
    "\n",
    "# Save final model and metrics\n",
    "torch.save(model_cbam_loaded.state_dict(), os.path.join(CONFIG['checkpoint_dir'], 'cbam_final.pt'))\n",
    "with open(os.path.join(CONFIG['checkpoint_dir'], 'cbam_final_metrics.json'), 'w') as f:\n",
    "    json.dump(val_metrics_cbam, f, indent=2)\n",
    "print(\"✓ Final model and metrics saved for reproducibility\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
